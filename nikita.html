<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Paper Template - CS 3043</title>
  <link rel="stylesheet" href="style.css">
  <link rel="stylesheet" href="article.css">
</head>
<body>
  <div class="navbar">
    <input type="checkbox" id="nav-toggle" class="nav-toggle" aria-label="Toggle navigation">
    <label for="nav-toggle" class="hamburger" aria-label="Open menu">☰ Menu</label>
    <a href="index.html" onclick="this.href = linkWithContext(this.getAttribute('href'))">Home</a>
    <a href="nikita.html" onclick="this.href = linkWithContext(this.getAttribute('href'))">Tracking Online and Data Safety</a>
    <a href="vu.html" onclick="this.href = linkWithContext(this.getAttribute('href'))">Globalization and Automation</a>
    <a href="abhi.html" onclick="this.href = linkWithContext(this.getAttribute('href'))">Policing and Tracking Using AI</a>
  </div>

  <div class="banner banner--article">
    <!-- Replace with your paper title -->
    <h1>Tracking <span class="highlight">Online</span> and Data Safety</h1>
  </div>

  <main class="container">
    <article class="paper">
      <!-- Replace author -->
      <p class="paper-author">By Nikita Ostapenko</p>
      <h1>The Ethics of Surveillance: Balancing Privacy and Security in the Digital Age</h1>

      <p>
        In democratic societies, privacy has long been viewed as a cornerstone of freedom—a safeguard for individual autonomy and a defense against the abuse of power. Yet in the age of smartphones, social media, and smart devices, protecting that privacy has become increasingly difficult. Governments and corporations now possess unprecedented capabilities to monitor, track, and analyze human behavior, raising critical ethical questions about the boundaries of surveillance.
      </p>

     
      <h2>Government Surveillance and the Erosion of Trust</h2>

      <p>
        Real-world cases make the ethical tension between safety and privacy all the more tangible. The United Kingdom’s <strong>Online Safety Act</strong>, for instance, was introduced to protect children from harmful online content. However, critics argue that the law’s broad scope could empower the government to censor political speech and enforce intrusive identity verification, undermining free expression in the process.
      </p>

      <p>
        Across the Atlantic, U.S. authorities have employed <strong>Palantir Technologies</strong>, a data analytics company known for its powerful tools that merge information from healthcare, financial, and criminal justice databases. While often justified in the name of national security, these tools allow for unprecedented insight into individuals’ lives—enabling predictive policing and mass surveillance that blur the line between safety and intrusion.
      </p>

      <p>
        Ethical analysis offers a revealing lens on these developments. From a <strong>Kantian perspective</strong>, both the Online Safety Act and Palantir’s surveillance systems treat individuals as means to an end—tools for achieving policy goals—rather than autonomous beings with inherent rights. <strong>Social contract theory</strong> warns that such overreach violates the trust citizens place in their governments. And while <strong>utilitarianism</strong> might defend surveillance in the name of public safety, the benefits often remain speculative, while the harms—chilling speech, weakening democracy, and eroding privacy—are immediate and profound.
      </p>

      <h2>Corporate Data and the Fragility of Privacy</h2>

      <p>
        The risks of data collection aren’t confined to governments. Corporate failures often expose how fragile privacy can be in a connected world. The <strong>Tea dating app</strong> breach, for example, twice left its database unsecured, leaking users’ photos, conversations, and personal details. What began as a platform for safety—helping women vet potential partners—became a cautionary tale about how easily trust can be broken.
      </p>

      <p>
        A similar concern surrounds the U.S. <strong>Kids Online Safety Act (KOSA)</strong>, a bill designed to protect minors from harmful online content. Critics argue that its “duty of care” requirements could lead to invasive age-verification systems and identity tracking. Worse, it could enable political censorship by labeling LGBTQ+ resources as “harmful.” What begins as protection risks becoming surveillance, wrapped in the language of safety.
      </p>

      <p>
        Viewed through the same ethical frameworks, both cases illustrate recurring failures. Kantian ethics condemns the reduction of individuals to data points; social contract theory highlights how negligence and overreach corrode trust; and utilitarianism questions whether speculative benefits truly outweigh the real harms of privacy loss and social exclusion.
      </p>

      <h2>Finding Balance: Safety Without Sacrificing Privacy</h2>

      <p>
        Despite the risks, few would argue for a world without any regulation or surveillance. The challenge lies in achieving balance. A report from <strong>The Brookings Institution</strong> suggests that effective safety measures need not come at the expense of privacy. Instead of requiring government-issued IDs for age verification, platforms could use <strong>privacy-preserving technologies</strong>—such as on-device content filtering or anonymous age estimation—to protect users while keeping their identities secure.
      </p>

      <p>
        This approach reflects a key ethical principle: transparency and minimal data collection must guide all surveillance and safety measures. In our own QR code experiment, for example, no data was stored or analyzed. The goal was simply to raise awareness—to show how easily technology can track individuals, and how ethical boundaries can be respected even in demonstrations of surveillance.
      </p>

      <h2>Why Privacy Still Matters</h2>

      <p>
        The growing power of governments and corporations to collect and interpret data poses one of the defining moral challenges of the digital era. Cases like the Online Safety Act, Palantir’s analytics, the Tea app breach, and KOSA all point to the same conclusion: when privacy is compromised, so too are autonomy, trust, and democracy itself.
      </p>

      <p>
        Technology is not inherently unethical—it’s the way it’s used that matters. As society continues to grapple with questions of security, safety, and control, we must remember that privacy is more than just a personal preference. It is the foundation of freedom, creativity, and human dignity in the digital world.
      </p>
    </article>
  </main>

  <script src="util.js"></script>
</body>
</html>

